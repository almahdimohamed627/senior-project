"""
QA chain logic: detects if query is dental, routes children, asks for triggers,
and uses LangChain (LLM + Chroma retriever) to triage to the closest specialty.
Optionally uses Tavily web-search fallback ONLY when retrieval yields no docs/empty context.
"""

import os
import re
from typing import List

from langchain_core.documents import Document
from langchain_ollama import OllamaLLM
from langchain_groq import ChatGroq
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_chroma import Chroma

from .reranker import rerank
from .web_search import tavily_search_documents


ARABIC_DIACRITICS = re.compile(r"[\u0617-\u061A\u064B-\u0652\u0670\u0640]")


def _normalize(text: str) -> str:
    """Normalize Arabic text for robust keyword detection."""
    text = (text or "").strip()
    text = ARABIC_DIACRITICS.sub("", text)
    text = (
        text.replace("Ø£", "Ø§")
        .replace("Ø¥", "Ø§")
        .replace("Ø¢", "Ø§")
        .replace("Ù‰", "ÙŠ")
        .replace("Ø¤", "Ùˆ")
        .replace("Ø¦", "ÙŠ")
        .replace("Ø©", "Ù‡")
        .lower()
    )
    text = re.sub(r"[^\w\s\u0600-\u06FF]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def _strip_al(token: str) -> str:
    """Remove Arabic definite article 'Ø§Ù„' prefix for matching."""
    token = (token or "").strip()
    if token.startswith("Ø§Ù„") and len(token) > 2:
        return token[2:]
    return token


def _token_variants(token: str) -> set[str]:
    """Return common Arabic clitic variants (Ø§Ù„/Ø¨Ø§Ù„/Ø¨) for matching."""
    t = (token or "").strip()
    out = {t, _strip_al(t)}
    if t.startswith("Ø¨Ø§Ù„") and len(t) > 3:
        out.add(t[3:])
        out.add(_strip_al(t[3:]))
    if t.startswith("Ø¨") and len(t) > 1:
        out.add(t[1:])
        out.add(_strip_al(t[1:]))
    return {x for x in out if x}


DENTAL_TOKEN_KEYWORDS = {
    # Dental / teeth
    "Ø³Ù†",
    "Ø³Ù†ÙŠ",
    "Ø§Ø³Ù†Ø§Ù†",
    "Ø¶Ø±Ø³",
    "Ø§Ø¶Ø±Ø§Ø³",
    "Ù„Ø«Ù‡",
    "Ø§Ù„Ù„Ø«Ù‡",
    "ØªØ³ÙˆØ³",
    "Ù†Ø®Ø±",
    "Ø·Ù‚Ù…",
    "Ø¬Ø³Ø±",
    "ØªÙ„Ø¨ÙŠØ³",
    "ØªØ§Ø¬",
    "Ø­Ø´ÙˆÙ‡",
    "Ø¹ØµØ¨",
    "Ø®Ø±Ø§Ø¬",
    "ØªÙˆØ±Ù…",
    "Ø­Ø³Ø§Ø³ÙŠÙ‡",
    "Ø­Ø³Ø§Ø³",
    "Ù„Ù…Ø¹Ù‡",
    # TMJ / orofacial pain
    "ÙÙƒ",
    "Ø§Ù„ÙÙƒ",
    "Ù…ÙØµÙ„",
    "Ø·Ù‚Ø·Ù‚Ù‡",
    "ØµØ±ÙŠØ±",
    "Ø·Ø­Ù†",
    "Ø´Ø¯",
    "ØªØ´Ù†Ø¬",
    "Ù…Ø¶Øº",
    "Ø¹Ø¶",
    "Ù‚ÙÙ„",
    "ØµØ¯Ø§Ø¹",
    "ØµØ¯Øº",

    "Ø³Ù†ÙŠÙ‡",
    "Ø§Ø³Ù†Ø§Ù†ÙŠ",
    "Ø¶Ø±Øµ",
    "Ø¶Ø±ÙˆØ³ÙŠ",}

DENTAL_PHRASE_KEYWORDS = {
    "Ø¶Ø±Ø³ Ø§Ù„Ø¹Ù‚Ù„",
    "ÙˆØ¬Ø¹ Ø³Ù†",
    "Ø§Ù„Ù… Ø³Ù†",
    "Ø§Ù„Ù… Ø¶Ø±Ø³",
    "ØªÙˆØ±Ù… Ø¨Ø§Ù„ÙˆØ¬Ù‡",
    "Ù…ÙØµÙ„ Ø§Ù„ÙÙƒ",
    "Ø§Ù„Ù… Ø§Ù„ÙÙƒ",
    "ÙØªØ­ Ø§Ù„ÙÙ…",
    "Ø·Ù‚Ø·Ù‚Ù‡ Ø§Ù„ÙÙƒ",
    "ØµØ±ÙŠØ± Ø§Ù„Ø§Ø³Ù†Ø§Ù†",
    "Ø·Ø­Ù† Ø§Ù„Ø§Ø³Ù†Ø§Ù†",

    "Ø¶Ø±Øµ Ø§Ù„Ø¹Ù‚Ù„",}

TRIGGER_TOKEN_KEYWORDS = {
    "Ø¨Ø§Ø±Ø¯",
    "Ø¨Ø§Ø±Ø¯Ù‡",
    "Ø¨Ø±Ø¯",
    "Ø­Ù„Ùˆ",
    "Ø­Ù„ÙˆÙ‡",
    "Ø­Ø§Ø±",
    "Ø­Ø§Ù…ÙŠ",
    "Ø­Ø±Ø§Ø±Ù‡",
    "Ø³Ø®Ù†",
    "Ø³Ø§Ø®Ù†",
    "Ø³Ø§Ø®Ù†Ù‡",
    "Ø¹ÙÙˆÙŠ",
    "Ø¹ÙÙˆÙŠÙ‡",
    "Ø¨Ø¯ÙˆÙ†",
    "Ø¯ÙˆÙ†",
    "Ø³Ø¨Ø¨",

    "Ø§Ù„Ø¨Ø§Ø±Ø¯",
    "Ø§Ù„Ø­Ø§Ø±",
    "Ø§Ù„Ø­Ù„Ùˆ",}

TRIGGER_PHRASES = {
    "Ø¨Ø¯ÙˆÙ† Ø³Ø¨Ø¨",
    "Ø¯ÙˆÙ† Ø³Ø¨Ø¨",
    "Ù…Ù† Ø¯ÙˆÙ† Ø³Ø¨Ø¨",
}

CHILD_PHRASES = {
    "Ø·ÙÙ„",
    "Ø·ÙÙ„Ù‡",
    "Ø§Ø¨Ù†ÙŠ",
    "Ø¨Ù†ÙŠ",
    "Ø§Ø¨Ù†ÙŠ Ø¹Ù…Ø±Ù‡",
    "Ø¨Ù†ØªÙŠ",
    "Ø·ÙÙ„ØªÙŠ",
    "ÙˆÙ„Ø¯ÙŠ",
}


def get_llm(backend: str = "groq"):
    """
    Return the configured LLM client; groq is default backend.
    Switch via env DENTAL_LLM_BACKEND=groq|ollama (defaults to groq).
    """
    backend = (backend or "groq").lower()

    if backend == "groq":
        api_key = os.getenv("GROQ_API_KEY", "").strip()
        if not api_key:
            raise ValueError("GROQ_API_KEY is not set in environment")

        model_name = os.getenv("GROQ_MODEL", "llama-3.1-70b-versatile")

        return ChatGroq(
            api_key=api_key,
            model_name=model_name,
            temperature=0.0,
        )

    if backend == "ollama":
        ollama_model = os.getenv("OLLAMA_LLM_MODEL", "llama3.1")
        return OllamaLLM(
            model=ollama_model,
            temperature=0.0,
        )

    raise ValueError(f"Unsupported LLM backend: {backend}")


def is_probably_dental(text: str) -> bool:
    """Detect if a message is likely dental-related (including TMJ/Bruxism)."""
    norm = _normalize(text)
    toks: set[str] = set()
    for t in norm.split():
        toks |= _token_variants(t)

    if toks & DENTAL_TOKEN_KEYWORDS:
        return True
    if any(p in norm for p in DENTAL_PHRASE_KEYWORDS):
        return True

    # Trigger words alone may still indicate dental sensitivity context
    if toks & TRIGGER_TOKEN_KEYWORDS:
        return True
    if any(p in norm for p in TRIGGER_PHRASES):
        return True

    return False


def _has_trigger(text: str) -> bool:
    norm = _normalize(text)
    toks: set[str] = set()
    for t in norm.split():
        toks |= _token_variants(t)
    if toks & TRIGGER_TOKEN_KEYWORDS:
        return True
    return any(p in norm for p in TRIGGER_PHRASES)


def _is_child(text: str) -> bool:
    norm = _normalize(text)
    return any(p in norm for p in CHILD_PHRASES)


def create_qa_chain(vectordb: Chroma, backend: str = "groq") -> RunnableLambda:
    rerank_enabled = (os.getenv("DENTAL_USE_RERANKER", "false") or "false").lower() in {
        "1",
        "true",
        "yes",
    }
    rerank_candidates = int(os.getenv("DENTAL_RERANK_CANDIDATES", "8"))
    rerank_topk = int(os.getenv("DENTAL_RERANK_TOPK", "4"))
    rerank_model = os.getenv(
        "DENTAL_RERANK_MODEL", "Omartificial-Intelligence-Space/ARA-Reranker-V1"
    )
    if rerank_enabled:
        print(f"ğŸ” Reranker enabled: {rerank_model}")

    web_fallback_enabled = (os.getenv("DENTAL_USE_WEB_FALLBACK", "false") or "false").lower() in {
        "1",
        "true",
        "yes",
    }

    retriever = vectordb.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"k": rerank_candidates, "score_threshold": 0.4},
    )

    llm = get_llm(backend=backend)
    parser = StrOutputParser()

    rewrite_prompt = PromptTemplate(
        template=(
            "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù„Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø´ÙƒÙˆÙ‰ Ù…Ø±ÙŠØ¶ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†.\n"
            "Ø­ÙˆÙ‘Ù„ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ù„Ù‰ ÙˆØµÙ Ø·Ø¨ÙŠ Ù…Ø®ØªØµØ± ÙˆÙˆØ§Ø¶Ø­ØŒ "
            "Ø¨Ø¯ÙˆÙ† Ø¥Ø¶Ø§ÙØ© Ø£Ø¹Ø±Ø§Ø¶ Ø¬Ø¯ÙŠØ¯Ø© ØºÙŠØ± Ù…Ø°ÙƒÙˆØ±Ø©ØŒ ÙˆØ¨Ø¯ÙˆÙ† ØªØºÙŠÙŠØ± Ø§Ù„Ù…Ø¹Ù†Ù‰.\n\n"
            "Ø´ÙƒÙˆÙ‰ Ø§Ù„Ù…Ø±ÙŠØ¶:\n{question}\n\n"
            "Ø§Ù„ÙˆØµÙ Ø§Ù„Ø·Ø¨ÙŠ Ø§Ù„Ù…Ø®ØªØµØ±:"
        ),
        input_variables=["question"],
    )
    rewrite_chain = rewrite_prompt | llm | parser

    triage_prompt = PromptTemplate(
        template=(
            "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ ÙØ±Ø² (triage) ØªÙØ§Ø¹Ù„ÙŠ ÙÙŠ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†.\n"
            "Ù…Ù‡Ù…ØªÙƒ Ù‚Ø±Ø§Ø¡Ø© Ø´ÙƒÙˆÙ‰ Ø§Ù„Ù…Ø±ÙŠØ¶ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø·Ø¨ÙŠ Ø§Ù„Ù…Ø±ÙÙ‚ Ù„ØªØ­Ø¯ÙŠØ¯ Ø£Ù‚Ø±Ø¨ Ø§Ø®ØªØµØ§Øµ Ø£Ø³Ù†Ø§Ù† Ù„Ù„Ø¨Ø§Ù„ØºÙŠÙ† "
            "(ØªØ±Ù…ÙŠÙ…ÙŠØ©/Ù„Ø¨ÙŠØ©/Ù„Ø«ÙˆÙŠØ©/ØªØ¹ÙˆÙŠØ¶Ø§Øª Ø«Ø§Ø¨ØªØ©/ØªØ¹ÙˆÙŠØ¶Ø§Øª Ù…ØªØ­Ø±ÙƒØ©)ØŒ "
            "ÙˆØ·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ§Ø¨Ø¹Ø© Ù…Ø­Ø¯Ø¯Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†Ø§Ù‚ØµØ©.\n\n"
            "Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø£Ø·ÙØ§Ù„:\n"
            "- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¹Ù…Ø± Ø§Ù„Ù…Ø±ÙŠØ¶ Ø£Ù‚Ù„ Ù…Ù† 13 Ø³Ù†Ø© Ø£Ùˆ ÙƒØ§Ù† ÙˆØ§Ø¶Ø­Ø§Ù‹ Ø£Ù†Ù‡ Ø·ÙÙ„ â†’ Ø­ÙˆÙ„ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ù„Ù‰ Ø£Ø³Ù†Ø§Ù† Ø£Ø·ÙØ§Ù„ "
            "ÙˆÙ„Ø§ ØªØ³ØªØ¹Ø±Ø¶ Ø§Ø®ØªØµØ§ØµØ§Øª Ø§Ù„Ø¨Ø§Ù„ØºÙŠÙ†.\n\n"
            "Ø§Ù„Ø§Ø®ØªØµØ§ØµØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„Ø¨Ø§Ù„ØºÙŠÙ† ÙÙ‚Ø·:\n"
            "- ØªØ±Ù…ÙŠÙ…ÙŠØ©\n"
            "- Ù„Ø¨ÙŠØ©\n"
            "- Ù„Ø«ÙˆÙŠØ©\n"
            "- ØªØ¹ÙˆÙŠØ¶Ø§Øª Ø«Ø§Ø¨ØªØ©\n"
            "- ØªØ¹ÙˆÙŠØ¶Ø§Øª Ù…ØªØ­Ø±ÙƒØ©\n\n"
            "Ù‚ÙˆØ§Ø¹Ø¯ Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø¹Ù…Ø±:\n"
            "- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¹Ù…Ø± Ø§Ù„Ù…Ø±ÙŠØ¶ Ø£Ù‚Ù„ Ù…Ù† 13 Ø³Ù†Ø© Ø£Ùˆ ÙƒØ§Ù† ÙˆØ§Ø¶Ø­Ø§Ù‹ Ø£Ù†Ù‡ Ø·ÙÙ„ØŒ ÙØ§Ù„Ø§Ø®ØªØµØ§Øµ: Ø£Ø³Ù†Ø§Ù† Ø£Ø·ÙØ§Ù„ (ØªØ­ÙˆÙŠÙ„).\n"
            "- Ø¥Ø°Ø§ Ù„Ù… ÙŠÙØ°ÙƒØ± Ø§Ù„Ø¹Ù…Ø±ØŒ Ø­Ù„Ù‘Ù„ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø§Ø®ØªØ±Ø§Ø¹ Ø¹Ù…Ø±.\n\n"
            "Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø³Ø±ÙŠØ¹Ø© Ù„Ù„ØªÙ…ÙŠÙŠØ² Ø¨ÙŠÙ† Ø§Ù„Ø­Ø§Ù„Ø§Øª:\n"
            "- Ø­Ø³Ø§Ø³ÙŠØ©/ØªØ±Ù…ÙŠÙ…ÙŠØ©: Ù„Ù…Ø¹Ø© Ø£Ùˆ Ø£Ù„Ù… Ø®ÙÙŠÙ/Ø­Ø§Ø¯ Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ Ù…Ø¹ Ø§Ù„Ø¨Ø§Ø±Ø¯ Ø£Ùˆ Ø§Ù„Ø­Ø§Ø± Ø£Ùˆ Ø§Ù„Ø­Ù„Ùˆ Ø£Ùˆ Ø§Ù„Ø­Ø§Ù…Ø¶ØŒ "
            "ÙŠØ®ØªÙÙŠ ÙÙˆØ± Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø¤Ø«Ù‘Ø±ØŒ Ø¨Ø¯ÙˆÙ† Ø£Ù„Ù… ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙˆØ¨Ø¯ÙˆÙ† Ø£Ù„Ù… ÙŠÙˆÙ‚Ø¸ Ø§Ù„Ù…Ø±ÙŠØ¶ Ù…Ù† Ø§Ù„Ù†ÙˆÙ… â†’ ÙŠØ±Ø¬Ù‘Ø­ Ø§Ø®ØªØµØ§Øµ ØªØ±Ù…ÙŠÙ…ÙŠØ© "
            "(Ø£Ùˆ Ù…Ø¹ Ù„Ø«ÙˆÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù†Ø­Ø³Ø§Ø± Ù„Ø«Ø© Ø£Ùˆ ØªØ¹Ø±Ù‘ÙŠ Ø¹Ù†Ù‚ Ø§Ù„Ø³Ù†).\n"
            "- Ø­Ø§Ù„Ø© Ù„Ø¨ÙŠÙ‘Ø© ØºÙŠØ± Ø¹ÙƒÙˆØ³Ø©: Ø£Ù„Ù… Ù‚ÙˆÙŠ Ø£Ùˆ Ù†Ø§Ø¨Ø¶ Ù…Ø¹ Ø§Ù„Ø¨Ø§Ø±Ø¯ Ø£Ùˆ Ø§Ù„Ø­Ø§Ø± ÙŠØ³ØªÙ…Ø± Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø¤Ø«Ù‘Ø±ØŒ Ø£Ùˆ Ø£Ù„Ù… ØªÙ„Ù‚Ø§Ø¦ÙŠ "
            "ÙŠÙˆÙ‚Ø¸ Ø§Ù„Ù…Ø±ÙŠØ¶ Ù…Ù† Ø§Ù„Ù†ÙˆÙ…ØŒ Ø£Ùˆ Ø£Ù„Ù… Ø´Ø¯ÙŠØ¯ Ø¹Ù†Ø¯ Ø§Ù„Ù‚Ø±Ø¹ Ø£Ùˆ Ø§Ù„Ù…Ø¶ØºØŒ Ø£Ùˆ ÙˆØ¬ÙˆØ¯ ØªÙˆØ±Ù‘Ù…/Ø®Ø±Ø§Ø¬ â†’ ÙŠØ±Ø¬Ù‘Ø­ Ø§Ø®ØªØµØ§Øµ Ù„Ø¨ÙŠØ©.\n\n"
            "Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©:\n"
            "- Ø§Ø¹ØªÙ…Ø¯ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ¹Ù„Ù‰ Ø´ÙƒÙˆÙ‰ Ø§Ù„Ù…Ø±ÙŠØ¶.\n"
            "- Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¥Ù„Ø§ ÙƒÙ…Ø¹Ø±ÙØ© Ø¹Ø§Ù…Ø© Ø¨Ø³ÙŠØ·Ø©.\n"
            "- Ù„Ø§ ØªÙØªØ±Ø¶ Ù…Ø­ÙÙ‘Ø²Ø§Ù‹ Ø£Ùˆ Ù…Ø¯Ø© Ø£Ùˆ Ø´Ø¯Ø© Ø¥Ø°Ø§ Ù„Ù… ØªÙØ°ÙƒØ± ØµØ±Ø§Ø­Ø©.\n"
            "- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†Ø§Ù‚ØµØ© ÙÙ„Ø§ ØªØ­Ø³Ù… Ø§Ù„Ø§Ø®ØªØµØ§Øµ Ù…Ø¨Ø§Ø´Ø±Ø©Ø› Ù‚Ø¯Ù‘Ù… ØªØ±Ø¬ÙŠØ­Ø§Ù‹ Ù…Ø´Ø±ÙˆØ·Ø§Ù‹ ÙˆØ§Ø¶Ø­Ø§Ù‹.\n"
            "- ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ø®ØªØ± Ø§Ø®ØªØµØ§ØµØ§Ù‹ ÙˆØ§Ø­Ø¯Ø§Ù‹ Ù„Ù„Ø¨Ø§Ù„ØºÙŠÙ†ØŒ Ù…Ø¹ Ø°ÙƒØ± Ø§Ù„Ø´Ø±Ø· Ø§Ù„Ø°ÙŠ ÙŠØºÙŠÙ‘Ø± Ø§Ù„Ø§Ø®ØªØµØ§Øµ Ø¥Ø°Ø§ Ù„Ø²Ù….\n"
            "- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø© ØªÙ…Ø§Ù…Ø§Ù‹ØŒ Ø£Ø¹Ø·Ù Ø£ÙØ¶Ù„ ØªØ®Ù…ÙŠÙ† Ù…Ø¤Ù‚Øª Ù…Ø¹ Ø§Ù„Ø³Ø¨Ø¨ØŒ ÙˆØ§Ø·Ø±Ø­ 2-3 Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ§Ø¨Ø¹Ø© Ù…Ø­Ø¯Ø¯Ø©.\n"
            "- Ù†Ø¨Ø±Ø© ÙˆØ¯ÙˆØ¯Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø©ØŒ Ø±Ø¯Ù‘ Ù…Ø®ØªØµØ± Ø«Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.\n\n"
            "Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø·Ø¨ÙŠ (Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©):\n{context}\n\n"
            "Ø´ÙƒÙˆÙ‰ Ø§Ù„Ù…Ø±ÙŠØ¶ Ø£Ùˆ Ø³Ø¤Ø§Ù„Ù‡:\n{question}\n\n"
            "Ø£Ø¹Ø·Ù Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ:\n"
            "Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ø®ØªØµØ±:\n"
            "- ...\n\n"
            "Ø§Ù„Ø§Ø®ØªØµØ§Øµ Ø§Ù„Ø£Ù†Ø³Ø¨ (Ù„Ù„Ø¨Ø§Ù„ØºÙŠÙ† ÙÙ‚Ø·ØŒ ÙˆÙ…Ø´Ø±ÙˆØ· Ø¥Ø°Ø§ Ù„Ø²Ù…):\n"
            "- ...\n\n"
            "Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ§Ø¨Ø¹Ø© Ø³Ø±ÙŠØ¹Ø© (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ØºÙ…ÙˆØ¶):\n"
            "- ...\n"
            "- ...\n"
            "- ..."
        ),
        input_variables=["context", "question"],
    )

    web_triage_prompt = PromptTemplate(
        template=(
            "Ù…Ù„Ø§Ø­Ø¸Ø©: Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ù‚ØªØ·ÙØ§Øª ÙˆÙŠØ¨ Ø¹Ø§Ù…Ø© ÙˆÙ„ÙŠØ³Øª ØªØ´Ø®ÙŠØµØ§Ù‹.\n"
            "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ ÙØ±Ø² Ø£ÙˆÙ„ÙŠ ÙÙŠ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†.\n\n"
            "Ø§Ø®ØªØ± Ø§Ø®ØªØµØ§ØµØ§Ù‹ ÙˆØ§Ø­Ø¯Ø§Ù‹ ÙÙ‚Ø· Ù…Ù†:\n"
            "- ØªØ±Ù…ÙŠÙ…ÙŠØ©\n"
            "- Ù„Ø¨ÙŠØ©\n"
            "- Ù„Ø«ÙˆÙŠØ©\n"
            "- ØªØ¹ÙˆÙŠØ¶Ø§Øª Ø«Ø§Ø¨ØªØ©\n"
            "- ØªØ¹ÙˆÙŠØ¶Ø§Øª Ù…ØªØ­Ø±ÙƒØ©\n\n"
            "Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©ØŒ Ø§Ø³Ø£Ù„ 2-3 Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ§Ø¨Ø¹Ø© Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø¯ÙˆÙ† Ø§Ø®ØªÙŠØ§Ø± Ù†Ù‡Ø§Ø¦ÙŠ.\n"
            "Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø­Ù‚Ø§Ø¦Ù‚ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø´ÙƒÙˆÙ‰ Ø£Ùˆ Ø§Ù„Ù…Ù‚ØªØ·ÙØ§Øª.\n\n"
            "Ø§Ù„Ø³ÙŠØ§Ù‚:\n{context}\n\n"
            "Ø§Ù„Ø´ÙƒÙˆÙ‰:\n{question}\n\n"
            "Ø§Ù„Ø±Ø¯:"
        ),
        input_variables=["context", "question"],
    )

    general_prompt = PromptTemplate(
        template=(
            "Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙƒØªØ¨ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© (Ù‚Ø¯ Ù„Ø§ ØªÙƒÙˆÙ† Ø¹Ù† Ø§Ù„Ø£Ø³Ù†Ø§Ù†):\n\n"
            "{question}\n\n"
            "Ø±Ø¯Ù‘ Ø¹Ù„ÙŠÙ‡ Ø¨Ø£Ø³Ù„ÙˆØ¨ ÙˆØ¯ÙˆØ¯ ÙˆØ¨Ø³ÙŠØ· Ø¨Ø¬Ù…Ù„Ø© Ø£Ùˆ Ø¬Ù…Ù„ØªÙŠÙ†ØŒ "
            "Ø«Ù… Ø£ÙˆØ¶Ø­ Ù„Ù‡ Ø£Ù† Ø¯ÙˆØ±Ùƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù‡Ùˆ Ù…Ø³Ø§Ø¹Ø¯ ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠ Ù„ÙØ±Ø² Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø£Ø³Ù†Ø§Ù† "
            "(Ø£Ù„Ù… Ø§Ù„Ø£Ø³Ù†Ø§Ù†ØŒ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ©ØŒ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù„Ø«Ø©ØŒ Ø§Ù„ØªØ¹ÙˆÙŠØ¶Ø§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© ÙˆØ§Ù„Ù…ØªØ­Ø±ÙƒØ©ØŒ Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø£Ø·ÙØ§Ù„). "
            "ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ø·Ù„Ø¨ Ù…Ù†Ù‡ Ø£Ù† ÙŠØµÙ Ù„Ùƒ Ø£ÙŠ Ù…Ø´ÙƒÙ„Ø© Ø³Ù†ÙŠØ© Ù„Ùˆ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©."
        ),
        input_variables=["question"],
    )

    triage_chain = triage_prompt | llm | parser
    web_triage_chain = web_triage_prompt | llm | parser
    general_chain = general_prompt | llm | parser

    def _run(inputs: dict) -> dict:
        question = inputs.get("query") or inputs.get("question")
        age = inputs.get("age")
        if not question:
            raise ValueError("query/question is required")

        # 1) Non-dental route
        if not is_probably_dental(question):
            answer = general_chain.invoke({"question": question})
            return {"result": answer, "source_documents": []}

        # 2) Child route
        if (age is not None and age < 13) or _is_child(question):
            child_msg = (
                "ÙŠÙØ­ÙˆÙ‘ÙÙ„ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ù„Ù‰ Ø§Ø®ØªØµØ§Øµ Ø£Ø³Ù†Ø§Ù† Ø£Ø·ÙØ§Ù„ "
                + (f"(Ø§Ù„Ø¹Ù…Ø±: {age} Ø³Ù†Ø©). " if age is not None else "")
                + "ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ù…Ø¹ Ø·Ø¨ÙŠØ¨ Ø£Ø³Ù†Ø§Ù† Ø£Ø·ÙØ§Ù„."
            )
            return {"result": child_msg.strip(), "source_documents": []}

        # 3) "Ù„Ù…Ø¹Ø©" needs trigger clarification
        norm_q = _normalize(question)
        if ("Ù„Ù…Ø¹Ù‡" in norm_q.split() or "Ù„Ù…Ø¹Ø©" in question) and not _has_trigger(question):
            ask_trigger = (
                "Ø£Ù‡Ù„Ø§Ù‹! Ù„Ù†Ø­Ø¯Ø¯ Ø§Ù„Ø§Ø®ØªØµØ§Øµ Ø¨Ø¯Ù‚Ø© Ù„Ø§Ø²Ù… Ø£Ø¹Ø±Ù Ù…Ø­ÙÙ‘Ø² Ø§Ù„Ù„Ù…Ø¹Ø©:\n"
                "- Ù‡Ù„ ØªØ£ØªÙŠ Ù…Ø¹ Ø§Ù„Ø¨Ø§Ø±Ø¯ØŸ\n"
                "- Ù…Ø¹ Ø§Ù„Ø­Ù„ÙˆØŸ\n"
                "- Ù…Ø¹ Ø§Ù„Ø­Ø§Ø±/Ø§Ù„Ø³Ø®Ù†ØŸ\n"
                "- Ø£Ù… Ø¨Ø¯ÙˆÙ† Ø³Ø¨Ø¨ ÙˆØ§Ø¶Ø­ (Ø¹ÙÙˆÙŠØ©)ØŸ\n"
                "Ø£Ø®Ø¨Ø±Ù†ÙŠ Ø£ÙŠØ¶Ø§Ù‹ Ø¹Ù† Ù…Ø¯Ø© Ø§Ù„Ø£Ù„Ù… Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø­ÙÙ‘Ø²."
            )
            return {"result": ask_trigger, "source_documents": []}

        # 4) Rewrite -> retrieve
        rewritten = rewrite_chain.invoke({"question": question})
        docs = retriever.invoke(rewritten)

        # 5) Optional rerank
        if rerank_enabled and docs:
            before = len(docs)
            docs = rerank(question, docs, top_k=rerank_topk)
            print(f"ğŸ” Reranked docs: before={before}, after={len(docs)}")

        context = "\n\n".join(doc.page_content for doc in docs)

        # 6) Optional web fallback ONLY when Chroma retrieval is empty
        if web_fallback_enabled and (not docs or not context.strip()):
            web_docs: List[Document] = tavily_search_documents(question)
            if web_docs:
                web_context_parts: List[str] = []
                for i, doc in enumerate(web_docs, start=1):
                    title = doc.metadata.get("title") or "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
                    source = doc.metadata.get("source", "unknown")
                    snippet = (doc.page_content or "").strip()
                    web_context_parts.append(
                        f"[{i}] {title}\n{snippet}\nØ§Ù„Ù…ØµØ¯Ø±: {source}"
                    )
                web_context = "\n\n".join(web_context_parts)
                answer = web_triage_chain.invoke({"context": web_context, "question": question})
                return {"result": answer, "source_documents": web_docs}

        # 7) No docs fallback (local)
        if not docs or not context.strip():
            fallback = (
                "Ø£Ù‡Ù„Ø§Ù‹! Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ ÙØ±Ø² Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø£Ø³Ù†Ø§Ù†. "
                "Ø­Ø§ÙˆÙ„ ØªÙˆØµÙÙ„ÙŠ Ø£ÙƒØ«Ø±: Ø£ÙŠÙ† Ù…ÙƒØ§Ù† Ø§Ù„Ø£Ù„Ù… Ø¨Ø§Ù„Ø¶Ø¨Ø·ØŸ Ù…Ù†Ø° Ù…ØªÙ‰ Ø¨Ø¯Ø£ØŸ "
                "Ù‡Ù„ ÙŠØ²Ø¯Ø§Ø¯ Ù…Ø¹ Ø§Ù„Ø¨Ø§Ø±Ø¯ Ø£Ùˆ Ø§Ù„Ø­Ø§Ø± Ø£Ùˆ Ø¹Ù†Ø¯ Ø§Ù„Ø¹Ø¶ØŸ ÙˆÙ‡Ù„ ÙŠÙˆØ¬Ø¯ ØªÙˆØ±Ù‘Ù… Ø£Ùˆ Ù†Ø²Ù Ø£Ùˆ Ø­Ø±Ø§Ø±Ø© Ø¹Ø§Ù…Ø©ØŸ"
            )
            return {"result": fallback, "source_documents": []}

        # 8) Normal RAG triage
        answer = triage_chain.invoke({"context": context, "question": question})
        return {"result": answer, "source_documents": docs}

    return RunnableLambda(_run)
