"""QA chain logic: detects if query is dental, routes children, asks for triggers,
and uses LangChain (Ollama LLM + Chroma retriever) to triage to the closest specialty."""

import os

from langchain_ollama import OllamaLLM
from langchain_groq import ChatGroq
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_chroma import Chroma

from .reranker import rerank

LLM_MODEL_NAME = "llama3.1"

DENTAL_KEYWORDS = [
    "ุณู",
    "ุณูู",
    "ุงุณูุงู",
    "ุฃุณูุงู",
    "ุถุฑุณ",
    "ุฃุถุฑุงุณ",
    "ุถุฑุณ ุงูุนูู",
    "ูุซุฉ",
    "ุงููุซุฉ",
    "ุชุณูุณ",
    "ูุฎุฑ",
    "ุทูู",
    "ุฌุณุฑ",
    "ุชูุจูุณ",
    "ุชุงุฌ",
    "ุญุดูุฉ",
    "ุนุตุจ",
    "ูุฌุน ุณู",
    "ุฃูู ุณู",
    "ุฃูู ุถุฑุณ",
    "ุฎุฑุงุฌ",
    "ุชูุฑู ุจุงููุฌู",
    "ุญุณุงุณูุฉ",
    "ุญุณุงุณ",
    "ููุนุฉ",
]

TRIGGER_KEYWORDS = [
    "ุจุงุฑุฏ",
    "ุจุงุฑุฏุฉ",
    "ุจุฑุฏ",
    "ุญูู",
    "ุญููุฉ",
    "ุญุงุฑ",
    "ุญุงูู",
    "ุญุฑุงุฑุฉ",
    "ุณุฎู",
    "ุณุงุฎู",
    "ุณุงุฎูุฉ",
    "ุจุฏูู ุณุจุจ",
    "ุฏูู ุณุจุจ",
    "ูู ุฏูู ุณุจุจ",
    "ุนููู",
    "ุนูููุฉ",
]

CHILD_KEYWORDS = [
    "ุทูู",
    "ุทููุฉ",
    "ุงุจูู",
    "ุจูู",
    "ุงุจูู ุนูุฑู",
    "ุจูุชู",
    "ุทููุชู",
    "ููุฏู",
]


def get_llm(backend: str = "groq"):
    """
    Return the configured LLM client; groq is default backend.
    Switch via env DENTAL_LLM_BACKEND=groq|ollama (defaults to groq).
    """
    backend = (backend or "groq").lower()

    if backend == "groq":
        api_key = os.getenv("GROQ_API_KEY", "").strip()
        if not api_key:
            raise ValueError("GROQ_API_KEY is not set in environment")

        model_name = os.getenv("GROQ_MODEL", "llama-3.1-70b-versatile")

        return ChatGroq(
            api_key=api_key,
            model_name=model_name,
            temperature=0.0,
        )

    elif backend == "ollama":
        ollama_model = os.getenv("OLLAMA_LLM_MODEL", "llama3.1")
        return OllamaLLM(
            model=ollama_model,
            temperature=0.0,
        )

    else:
        raise ValueError(f"Unsupported LLM backend: {backend}")


def _normalize(text: str) -> str:
    return (
        text.replace("ุฃ", "ุง")
        .replace("ุฅ", "ุง")
        .replace("ุข", "ุง")
        .lower()
    )


def is_probably_dental(text: str) -> bool:
    norm = _normalize(text)
    return any(kw in norm for kw in DENTAL_KEYWORDS) or any(
        t in norm for t in TRIGGER_KEYWORDS
    )


def _has_trigger(text: str) -> bool:
    norm = _normalize(text)
    return any(t in norm for t in TRIGGER_KEYWORDS)


def _is_child(text: str) -> bool:
    norm = _normalize(text)
    return any(k in norm for k in CHILD_KEYWORDS)


def create_qa_chain(vectordb: Chroma, backend: str = "groq") -> RunnableLambda:
    rerank_enabled = (os.getenv("DENTAL_USE_RERANKER", "false") or "false").lower() in {
        "1",
        "true",
        "yes",
    }
    rerank_candidates = int(os.getenv("DENTAL_RERANK_CANDIDATES", "8"))
    rerank_topk = int(os.getenv("DENTAL_RERANK_TOPK", "4"))
    rerank_model = os.getenv(
        "DENTAL_RERANK_MODEL", "Omartificial-Intelligence-Space/ARA-Reranker-V1"
    )
    if rerank_enabled:
        print(f"๐ Reranker enabled: {rerank_model}")

    retriever = vectordb.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"k": rerank_candidates, "score_threshold": 0.4},
    )

    llm = get_llm(backend=backend)
    parser = StrOutputParser()

    rewrite_prompt = PromptTemplate(
        template=(
            "ุฃูุช ูุณุงุนุฏ ูุฅุนุงุฏุฉ ุตูุงุบุฉ ุดููู ูุฑูุถ ูู ูุฌุงู ุทุจ ุงูุฃุณูุงู.\n"
            "ุญููู ุงููุต ุงูุชุงูู ุฅูู ูุตู ุทุจู ูุฎุชุตุฑ ููุงุถุญุ "
            "ุจุฏูู ุฅุถุงูุฉ ุฃุนุฑุงุถ ุฌุฏูุฏุฉ ุบูุฑ ูุฐููุฑุฉุ ูุจุฏูู ุชุบููุฑ ุงููุนูู.\n\n"
            "ุดููู ุงููุฑูุถ:\n{question}\n\n"
            "ุงููุตู ุงูุทุจู ุงููุฎุชุตุฑ:"
        ),
        input_variables=["question"],
    )
    rewrite_chain = rewrite_prompt | llm | parser

    triage_prompt = PromptTemplate(
        template=(
            "ุฃูุช ูุณุงุนุฏ ูุฑุฒ (triage) ุชูุงุนูู ูู ุทุจ ุงูุฃุณูุงู.\n"
            "ูููุชู ูุฑุงุกุฉ ุดููู ุงููุฑูุถ ูุงุณุชุฎุฏุงู ุงูุณูุงู ุงูุทุจู ุงููุฑูู ูุชุญุฏูุฏ ุฃูุฑุจ ุงุฎุชุตุงุต ุฃุณูุงู ููุจุงูุบูู "
            "(ุชุฑููููุฉ/ูุจูุฉ/ูุซููุฉ/ุชุนููุถุงุช ุซุงุจุชุฉ/ุชุนููุถุงุช ูุชุญุฑูุฉ)ุ "
            "ูุทุฑุญ ุฃุณุฆูุฉ ูุชุงุจุนุฉ ูุญุฏุฏุฉ ุฅุฐุง ูุงูุช ุงููุนูููุงุช ูุงูุตุฉ.\n\n"
            "ููุงุนุฏ ุงูุฃุทูุงู:\n"
            "- ุฅุฐุง ูุงู ุนูุฑ ุงููุฑูุถ ุฃูู ูู 13 ุณูุฉ ุฃู ูุงู ูุงุถุญุงู ุฃูู ุทูู โ ุญูู ูุจุงุดุฑุฉ ุฅูู ุฃุณูุงู ุฃุทูุงู "
            "ููุง ุชุณุชุนุฑุถ ุงุฎุชุตุงุตุงุช ุงูุจุงูุบูู.\n\n"
            "ุงูุงุฎุชุตุงุตุงุช ุงููุชุงุญุฉ ููุจุงูุบูู ููุท:\n"
            "- ุชุฑููููุฉ\n"
            "- ูุจูุฉ\n"
            "- ูุซููุฉ\n"
            "- ุชุนููุถุงุช ุซุงุจุชุฉ\n"
            "- ุชุนููุถุงุช ูุชุญุฑูุฉ\n\n"
            "ููุงุนุฏ ุฎุงุตุฉ ุจุงูุนูุฑ:\n"
            "- ุฅุฐุง ูุงู ุนูุฑ ุงููุฑูุถ ุฃูู ูู 13 ุณูุฉ ุฃู ูุงู ูุงุถุญุงู ุฃูู ุทููุ ูุงูุงุฎุชุตุงุต: ุฃุณูุงู ุฃุทูุงู (ุชุญููู).\n"
            "- ุฅุฐุง ูู ููุฐูุฑ ุงูุนูุฑุ ุญููู ุงูุฃุนุฑุงุถ ููุท ุจุฏูู ุงุฎุชุฑุงุน ุนูุฑ.\n\n"
            "ุฅุฑุดุงุฏุงุช ุณุฑูุนุฉ ููุชูููุฒ ุจูู ุงูุญุงูุงุช:\n"
            "- ุญุณุงุณูุฉ/ุชุฑููููุฉ: ููุนุฉ ุฃู ุฃูู ุฎููู/ุญุงุฏ ูุตูุฑ ุฌุฏุงู ูุน ุงูุจุงุฑุฏ ุฃู ุงูุญุงุฑ ุฃู ุงูุญูู ุฃู ุงูุญุงูุถุ "
            "ูุฎุชูู ููุฑ ุฅุฒุงูุฉ ุงููุคุซูุฑุ ุจุฏูู ุฃูู ุชููุงุฆู ูุจุฏูู ุฃูู ูููุธ ุงููุฑูุถ ูู ุงูููู โ ูุฑุฌูุญ ุงุฎุชุตุงุต ุชุฑููููุฉ "
            "(ุฃู ูุน ูุซููุฉ ุฅุฐุง ูุงู ุงูุณุจุจ ุงูุญุณุงุฑ ูุซุฉ ุฃู ุชุนุฑูู ุนูู ุงูุณู).\n"
            "- ุญุงูุฉ ูุจููุฉ ุบูุฑ ุนููุณุฉ: ุฃูู ููู ุฃู ูุงุจุถ ูุน ุงูุจุงุฑุฏ ุฃู ุงูุญุงุฑ ูุณุชูุฑ ุจุนุฏ ุฅุฒุงูุฉ ุงููุคุซูุฑุ ุฃู ุฃูู ุชููุงุฆู "
            "ูููุธ ุงููุฑูุถ ูู ุงููููุ ุฃู ุฃูู ุดุฏูุฏ ุนูุฏ ุงููุฑุน ุฃู ุงููุถุบุ ุฃู ูุฌูุฏ ุชูุฑูู/ุฎุฑุงุฌ โ ูุฑุฌูุญ ุงุฎุชุตุงุต ูุจูุฉ.\n\n"
            "ุงูุชุนูููุงุช ุงููููุฉ:\n"
            "- ุงุนุชูุฏ ููุท ุนูู ุงููุนูููุงุช ุงูููุฌูุฏุฉ ูู ุงูุณูุงู ูุนูู ุดููู ุงููุฑูุถ.\n"
            "- ูุง ุชุณุชุฎุฏู ูุนูููุงุช ูู ุฎุงุฑุฌ ุงูุณูุงู ุฅูุง ููุนุฑูุฉ ุนุงูุฉ ุจุณูุทุฉ.\n"
            "- ูุง ุชูุชุฑุถ ูุญููุฒุงู ุฃู ูุฏุฉ ุฃู ุดุฏุฉ ุฅุฐุง ูู ุชูุฐูุฑ ุตุฑุงุญุฉ. ุฅุฐุง ูุงูุช ุงูุดููู ุนุงูุฉ ูุซู 'ุนูุฏู ููุนุฉ'ุ ุตุฑูุญ ุฃู ุงููุญููุฒ/ุงููุฏุฉ ุบูุฑ ูุฐููุฑูู.\n"
            "- ุฅุฐุง ูุงูุช ุงููุนูููุงุช ูุงูุตุฉ ููุง ุชุญุณู ุงูุงุฎุชุตุงุต ูุจุงุดุฑุฉุ ูุฏูู ุชุฑุฌูุญุงู ูุดุฑูุทุงู ูุงุถุญุงู "
            "(ูุซูุงู: ุชุฑููููุฉ ุฅุฐุง ุงูููุนุฉ ูุตูุฑุฉ ูุน ุงูุจุงุฑุฏ/ุงูุญููุ ูุจูุฉ ุฅุฐุง ูุน ุงูุญุงุฑ ุฃู ูุณุชูุฑุฉ/ุชููุธ ูู ุงููููุ ูุซููุฉ ุฅุฐุง ุจุฏูู ูุญููุฒ ููุน ุงูุญุณุงุฑ ูุซุฉ).\n"
            "- ูู ุงูููุงูุฉ ุงุฎุชุฑ ุงุฎุชุตุงุตุงู ูุงุญุฏุงู ููุจุงูุบููุ ูุน ุฐูุฑ ุงูุดุฑุท ุงูุฐู ูุบููุฑ ุงูุงุฎุชุตุงุต ุฅุฐุง ูุฒู.\n"
            "- ุฅุฐุง ูุงูุช ุงูุฃุนุฑุงุถ ุบูุฑ ูุงุถุญุฉ ุชูุงูุงูุ ุฃุนุทู ุฃูุถู ุชุฎููู ูุคูุช ูุน ุงูุณุจุจุ ูุงุทุฑุญ 2-3 ุฃุณุฆูุฉ ูุชุงุจุนุฉ ูุญุฏุฏุฉ "
            "(ูุซู: ููุงู ุงูุฃููุ ูุฏุชูุ ูู ููุงู ุชูุฑู/ูุฒูุ ูู ุงูุฃูู ูุน ุงูุจุงุฑุฏ/ุงูุญุงุฑ/ุงูุนุถู).\n"
            "- ูุจุฑุฉ ูุฏูุฏุฉ ููุจุงุดุฑุฉุ ุฑุฏู ูุฎุชุตุฑ ุซู ุงูุฃุณุฆูุฉ.\n\n"
            "ุชุนูููุงุช ุญูู ุตูุงุบุฉ ุงูุฌูุงุจ:\n"
            "- ูุง ุชุนูุฏ ูุชุงุจุฉ ุดููู ุงููุฑูุถ ููุง ุชุบููุฑ ุชูุงุตูููุงุ ููุง ุชุถู ุฃูุซูุฉ ุฌุฏูุฏุฉ ูู ุนูุฏู.\n"
            "- ูุง ุชุฐูุฑ ูู ุงูุฌูุงุจ ุฃู ุงูุฃูู ูุฎุชูู ุฃู ูุณุชูุฑ ุฅูุง ุฅุฐุง ุฐููุฑ ุฐูู ุตุฑุงุญุฉ ูู ุงูุดููู.\n"
            "- ุฅุฐุง ูุงูุช ุงูุดููู ูุตูุฑุฉ ุฌุฏุงู ุฃู ุนุงูุฉุ ุตุฑูุญ ุจููุฉ ุงููุนูููุงุช ูุงุฐูุฑ ุงูุดุฑูุท ุงูุชู ุชุญุฏุฏ ุงูุงุฎุชุตุงุตุ ูุงุทูุจ ุงููุญููุฒ (ุจุงุฑุฏ/ุญูู/ุญุงุฑ/ุนููู) ูุงููุฏุฉ ุจุฏูุฉ.\n\n"
            "ุงูุณูุงู ุงูุทุจู (ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ):\n{context}\n\n"
            "ุดููู ุงููุฑูุถ ุฃู ุณุคุงูู:\n{question}\n\n"
            "ุฃุนุทู ุงูุฅุฌุงุจุฉ ุจุงูุชูุณูู ุงูุชุงูู:\n"
            "ุงูุฑุฏ ุงููุฎุชุตุฑ:\n"
            "- ุฌููุฉ ุฃู ุงุซูุชุงู ุชูุฎูุตุงู ุงูุญุงูุฉ ุจุฃูุฑุจ ุงุฎุชุตุงุต ูุญุชูู ูุน ุฐูุฑ ุงูุดุฑูุท ุจูุถูุญ.\n\n"
            "ุงูุงุฎุชุตุงุต ุงูุฃูุณุจ (ููุจุงูุบูู ููุทุ ููุดุฑูุท ุฅุฐุง ูุฒู):\n"
            "- ูุงุญุฏ ููุท ูู: ุชุฑููููุฉ / ูุจูุฉ / ูุซููุฉ / ุชุนููุถุงุช ุซุงุจุชุฉ / ุชุนููุถุงุช ูุชุญุฑูุฉุ "
            "ูุน ุฐูุฑ ุงูุดุฑุท ุงูุฐู ูุบููุฑ ุงูุงุฎุชุตุงุต ุฅู ูุฌุฏ. ุฅุฐุง ุงููุฑูุถ ุทูู (<13 ุฃู ูุงุถุญ ุทูู) ูุงูุงุฎุชุตุงุต: ุฃุณูุงู ุฃุทูุงู (ุชุญููู).\n\n"
            "ุฃุณุฆูุฉ ูุชุงุจุนุฉ ุณุฑูุนุฉ (ุฅุฐุง ูุงู ููุงู ุบููุถ):\n"
            "- ุณุคุงู 1\n"
            "- ุณุคุงู 2\n"
            "- ุณุคุงู 3"
        ),
        input_variables=["context", "question"],
    )

    general_prompt = PromptTemplate(
        template=(
            "ุงููุณุชุฎุฏู ูุชุจ ุงูุฑุณุงูุฉ ุงูุชุงููุฉ (ูุฏ ูุง ุชููู ุนู ุงูุฃุณูุงู):\n\n"
            "{question}\n\n"
            "ุฑุฏู ุนููู ุจุฃุณููุจ ูุฏูุฏ ูุจุณูุท ุจุฌููุฉ ุฃู ุฌููุชููุ "
            "ุซู ุฃูุถุญ ูู ุฃู ุฏูุฑู ุงูุฃุณุงุณู ูู ูุณุงุนุฏ ุชูููููุฌู ููุฑุฒ ุญุงูุงุช ุงูุฃุณูุงู "
            "(ุฃูู ุงูุฃุณูุงูุ ุงูุญุณุงุณูุฉุ ูุดุงูู ุงููุซุฉุ ุงูุชุนููุถุงุช ุงูุซุงุจุชุฉ ูุงููุชุญุฑูุฉุ ุฃุณูุงู ุงูุฃุทูุงู). "
            "ูู ุงูููุงูุฉ ุงุทูุจ ููู ุฃู ูุตู ูู ุฃู ูุดููุฉ ุณููุฉ ูู ูุงูุช ููุฌูุฏุฉ."
        ),
        input_variables=["question"],
    )

    triage_chain = triage_prompt | llm | parser
    general_chain = general_prompt | llm | parser

    def _run(inputs: dict) -> dict:
        question = inputs.get("query") or inputs.get("question")
        age = inputs.get("age")
        if not question:
            raise ValueError("query/question is required")

        if not is_probably_dental(question):
            answer = general_chain.invoke({"question": question})
            return {"result": answer, "source_documents": []}

        if (age is not None and age < 13) or _is_child(question):
            child_msg = (
                "ููุญูููู ูุจุงุดุฑุฉ ุฅูู ุงุฎุชุตุงุต ุฃุณูุงู ุฃุทูุงู "
                + (f"(ุงูุนูุฑ: {age} ุณูุฉ). " if age is not None else "")
                + "ูุฑุฌู ุงููุชุงุจุนุฉ ูุน ุทุจูุจ ุฃุณูุงู ุฃุทูุงู."
            )
            return {"result": child_msg.strip(), "source_documents": []}

        if "ููุนุฉ" in question and not _has_trigger(question):
            ask_trigger = (
                "ุฃููุงู! ููุญุฏุฏ ุงูุงุฎุชุตุงุต ุจุฏูุฉ ูุงุฒู ุฃุนุฑู ูุญููุฒ ุงูููุนุฉ:\n"
                "- ูู ุชุฃุชู ูุน ุงูุจุงุฑุฏุ\n"
                "- ูุน ุงูุญููุ\n"
                "- ูุน ุงูุญุงุฑ/ุงูุณุฎูุ\n"
                "- ุฃู ุจุฏูู ุณุจุจ ูุงุถุญ (ุนูููุฉ)ุ\n"
                "ุฃุฎุจุฑูู ุฃูุถุงู ุนู ูุฏุฉ ุงูุฃูู ุจุนุฏ ุงููุญููุฒ."
            )
            return {"result": ask_trigger, "source_documents": []}

        rewritten = rewrite_chain.invoke({"question": question})

        docs = retriever.invoke(rewritten)

        if rerank_enabled and docs:
            before = len(docs)
            docs = rerank(question, docs, top_k=rerank_topk)
            print(f"๐ Reranked docs: before={before}, after={len(docs)}")

        context = "\n\n".join(doc.page_content for doc in docs)

        if not docs or not context.strip():
            fallback = (
                "ุฃููุงู! ุฃูุง ูุณุงุนุฏ ูุฑุฒ ูุญุงูุงุช ุงูุฃุณูุงู. "
                "ุญุงูู ุชูุตููู ุฃูุซุฑ: ุฃูู ููุงู ุงูุฃูู ุจุงูุถุจุทุ ููุฐ ูุชู ุจุฏุฃุ "
                "ูู ูุฒุฏุงุฏ ูุน ุงูุจุงุฑุฏ ุฃู ุงูุญุงุฑ ุฃู ุนูุฏ ุงูุนุถุ ููู ููุฌุฏ ุชูุฑูู ุฃู ูุฒู ุฃู ุญุฑุงุฑุฉ ุนุงูุฉุ"
            )
            return {"result": fallback, "source_documents": []}

        answer = triage_chain.invoke({"context": context, "question": question})
        return {"result": answer, "source_documents": docs}

    return RunnableLambda(_run)
